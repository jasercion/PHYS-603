\documentclass{book}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{tabu}

\makeatletter
\def\lecture{\@ifnextchar[{\@lectureWith}{\@lectureWithout}}
\def\@lectureWith[#1]{\medbreak\refstepcounter{section}%
  \renewcommand{\leftmark}{Lecture \thesection}
  \noindent{\addcontentsline{toc}{section}{Lecture \thesection: #1\@addpunct{.}}%
  \sectionfont Lecture \thesection. #1\@addpunct{.}}\medbreak}
\def\@lectureWithout{\medbreak\refstepcounter{section}%
  \renewcommand{\leftmark}{Lecture \thesection}
  \noindent{\addcontentsline{toc}{section}{Lecture \thesection.}%
  \sectionfont Lecture \thesection.}\medbreak}
\makeatother
\newcommand{\pardif}[2]{\frac{\delta#1}{\delta#2}}
\newcommand{\secpardif}[2]{\frac{\delta^{2}#1}{\delta#2^{2}}}
\newcommand{\thermdif}[4]{\frac{\delta#1}{\delta#2}\vert_{#3,#4}}
\newcommand{\secthermdif}[4]{\frac{\delta^{2}#1}{\delta#2^{2}}\vert_{#3,#4}}
\newcommand{\dif}[2]{\frac{d#1}{d#2}}
\newcommand{\bltz}{k_{B}}
\newcommand{\sumser}[2]{\sum\limits_{#1}^{#2}}

\title{Lectures on Statistical Mechanics and Thermodynamics}
\author{Paolo Bedaque}
\begin{document}
\maketitle

\chapter{Foundations of Statistical Mechanics}

%\lecture
The object of Statistical Mechanics is to study systems with a large number of degrees of freedom by focusing solely on the macroscopic properties of the system.  

\begin{figure}
	\includegraphics[width=0.5\linewidth]{C:/Users/Joe/Documents/Stat_Mech_Notes/Images/mic_to_mac.png}
	\caption{Multiple microscopic states make up a single macroscopic state of a system.}
\end{figure}

Why, you may ask, are we only focusing on these macroscopic properties?  Surely, given enough information we could model any system by following its individual members!  This is not necessarily the case because the sheer volume of information necessary to do such a calculation is in fact an impediment to performing such a calculation.  Let's illustrate this using a simple example of a system consisting of 20 particles in the ground state.  Suppose, through divine intervention or otherwise, we have obtained the complete wave equation for each particle in the system.  Before we begin to calculate any properties of this system we must first store this information.  

\begin{figure}
\centering
	\begin{minipage}{0,49\textwidth}
		\frame{\includegraphics[width=0.5\linewidth]{C:/Users/Joe/Documents/Stat_Mech_Notes/Images/fig_1_1.png}}
		\caption{Wavefunction of our system in dimension $x_{1}$}
	\end{minipage}
%	\begin{minipage}{0.49\textwidth}
%		\frame{\includegraphics[\width=\linewidth]]{imagefile}}
%		\caption{test}
%	\end{minipage}
\label{fig:information_graph}
\end{figure}

As shown in figure 2, every particle wavefunction will be reasonably approximated by 10 points in each dimension.  Taking the 3-dimensional case, we find that the amount of memory we would need would be $(10)^{3N}*8\simeq10^{61}$ bytes.  We include the factor of 8 since each real number will be represented by 8 bytes.  Assuming we store all of this data on terabyte drives that have a storage capacity of $\frac{10^{12} bytes}{0.4 kgs}$, we find that the total mass of all of the drives required to store the information describing this 20 particle system to be on the order of $10^{49}$ kilograms, which is much much more than the mass of the Milky Way galaxy!  Clearly, it is impossible to store this much information, much less manipulate it.  Therefore, we employ statistics to deal with the system at a macroscopic level. 

\section{Hamiltonian Dynamics}

Consider a hypothetical system consisting of some number of particles which can be described by their individual positions, $q_{i}$ and their momentums, $p_{i}$.  We can indicate the paths these particles travel on a {\bf{Phase Space}} diagram.  

%Insert phase space figure

This diagram displays the evolution of the systems constituent particles as their positions and momentums change.  The evolution of this system is governed by the Hamilton Equations:  $\dif{q_{1}}{t} = \pardif{\mathcal{H}}{p_{i}}$ and $\dif{p_{i}}{t}=-\pardif{\mathcal{H}}{q_{i}}$  The evolution of $p$ and $q$ are clearly deterministic, therefore we can express these values as time-dependant functions.  $q_{i}(t), p_{i}(t) \rightarrow q_{i}(t=0)=q_{i}^{0}$ and $p_{i}(t=0)=p_{i}^0$

There are a couple of tricks we can employ to idealize the measurement process and simplify manipulations of these functions.

\section{Idealization of the Measurement Process}

Let's start by defining some value $\overline{f}$, which is the result of measuring function $f(p_{i},q_{i})$.  

$$\overline{f}=\lim_{T\to\infty}\frac{1}{T}\int_{\delta}^{\delta+T}dtf(q(t),p(t))=\widetilde{f}(q_{i}^0,p_{i}^0)$$

$T=$ duration of the measurement, $\delta=$ time the measurement starts, $\widetilde{f}=$ some function of the initial conditions.

This expression is an idealization of an arbitrary measurement.  In reality, the time T is much larger than the microscopic scales in which the functions of $q(t)$ and $p(t)$ vary.  For example, suppose the function $f$ is the pressure on the wall of a gas containing vessel.  The quantity $f(q(t),p(t))$ flucuates like the diagram below.

%Insert pressure fluctuation diagram

However, except in specialized cases, the measuring apparatus is not accurate enough the capture every tiny fluctuation of the force on the wall of the vessel.  Instead, it measures an average of $f(q(t),p(t))$ over a finite period of time:

%Insert modified pressure fluctuation diagram

Taking the $\lim_{T\to\infty}$ is a mathematical idealization of this averaging out of function fluctuations.  Returning to our analysis of the hypothetical measurement $\overline{f}$, we see that the function $\widetilde{f}(q_{i}^0,p_{i}^0)$ is not really a function of the initial condition.  Rather, it is a function of the \textit{trajectory} of the system evolution.  This may not be immediately apparent, but can be illustrated by choosing another initial point on the trajectory of our function $\widetilde{f}=(q_{i}^0,p_{i}^0)$.  As $T\to\infty$, we see that our new initial points $q_{i}^{0'}$ and $p_{i}^{0'}$ yield the same value of $\widetilde{f}$ as $q_{i}^0$ and $p_{i}^0$.  i.e., $\widetilde{f}(q_{i}^0,p_{i}^0)=\widetilde{f}(q_{i}^{0'},p_{i}^{0'})$  This is because the difference between the two trajectories is neglible compared to the infinite extended trajectories as $T\to\infty$.

%Insert trajectory phase diagram

Now, it could be possible that two different trajectories give different values for $\overline{f}$.

%Insert IC Trajectory phase diagram

However, it turns out that, for systems we are interested in, that it does not happen.  These systems have the property that any individual trajectory will pass through almost \textbf{every} point in phase space.  This is called the \textbf{Ergodic Property}.  

A couple of important things to keep in mind.  The assumption that the trajectory passes through every point in the phase space is contingent on the the assumption that these points have the same value of energy (and any other conserved quantities) as the initial point.  After all, these values are conserved by the Hamiltonian Flow.  The phrase "almost every point" also has a specific meaning, in that it is "almost every point" in the measure theory sense.

It is really difficult to prove that a realistic system has the ergodic property.  Only a few systems have actually been proven rigorously to be ergodic (for example, the Sinai Stadium).  Moving forward, we will operate on the assumption that the systems we are interested are, in fact, ergodic.    

To expand on the concept of ergodicity, it is very easy to find systems that are not ergodic.  Any conserved value restricts the trajectories to lie on a submanifold of the phase space.  If the only conserved quantities are the ones resulting from the standard symmetries (i.e. energy, momeentum, etc.) we can question the validity of ergodicity within the restricted subspace with constant values of these quantities.  There are systems with so many other conserved quantities that the subspace is one dimensional (integrable systems), thereby invalidating the ergodicity of the system.  The Integrable/Ergodic classification is also non-binary, there are intermediate types of systems that exist between the two extremes, such as mixing systems.

\section{Introduction to Thermodynamics}

At its core level, many thermodynamic problems can be phrased as a comparison between two states of a system.  The pre-evolutionary equilibrium state and the post-evolutionary equilibrium state.

%Insert Thermoproblem breakdown diagram.

Since every microstate of the system is equally probably, the final macroscopic state will be the one which corresponds to the largest number of microstates:

$$\Gamma(E_{1}',V_{1}',N_{1}',E_{2}',V_{2}',N_{2}')=\Gamma_{1}(E_{1}',V_{1}',N_{1}')\Gamma_{2}(E_{2}',V_{2}',N_{2}') $$

Where $\Gamma(E_{1}',V_{1}',N_{1}',E_{2}',V_{2}',N_{2}')$ is the number of states of the composite system, $\Gamma_{1}(E_{1}',V_{1}',N_{1}')$ is the number of microstates of subsystem 1, and $\Gamma_{2}(E_{2}',V_{2}',N_{2}')$ is the number of microstates of subsystem 2.

Let's look at the case where the wall between the two subsystems allows the exchange of energy but not of volume or particle number.  In this case we find:

\begin{multline}
E_{1}+E_{2}=E_{1}'+E_{2}' \\
V_{1}=V_{1}', V_{2}=V_{2}' \\
N_{1}=N_{1}', N_{2}=N_{2}' \\
\Gamma(E_{1}',E_{2}')=\Gamma_{1}(E_{1}')\Gamma_{2}(E_{2}')
\end{multline}

This implies that

$$\Gamma_{Max, E_{total} fixed}\Leftrightarrow\frac{1}{\Gamma_{1}}\dif{\Gamma_{1}(E_{1}')}{E_{1}'}-\frac{1}{\Gamma_{2}(E_{2}')}\dif{\Gamma_{2}(E_{2}')}{E_{2}'}=0$$  

So, knowing the total number of initial microstates of the two subsystems allows us to predict the final state of the overall system.  This is summed up in a very clean and useful fashion in the definition of \textbf{entropy}:

$$S=\bltz\ln(\Gamma)$$
   
Notice that entropy is extensive as long as the subsystems are large enough that the boundary effects are negligible.

\textit{How can Entropy grow?}

Within the constraints of the problem, there is an apparent paradox in staying that the entropy is maximized.  That's because, using Hamiltonian equations, we can show S is a constant.

Define $\rho(q,p,t=0)$ to be an initial distribution of identical systems (this is called an \textbf{ensemble}.)  As the Hamiltonian Flow evolves in time every element of the ensemble is carried with it and the distribution evolves to $\rho(q,p,t)=\rho(q(t),p(t))$.

%Insert flow evolution diagram

Because the number of systems in the ensemble is fixed $\rho$ obeys the continuity equation:

$$\dif{\rho}{t}+\nabla*(\rho\nabla)=0$$
or
\begin{equation}
\begin{split}
0=\pardif{\rho}{t}+\sum_{i=1}^{3N}[\pardif{(\rho\dot{q_{i}})}{q_{i}}+\pardif{\rho\dot{p_{i}}}{p_{i}}] \\
	=\pardif{\rho}{t}+
\end{split}	 
\end{equation}
%Finish Equation

This result is known as the \textbf{Liouville Theorem}.  It implies that the density of elements on the ensemble does not change as it evolves along the Hamiltonian Flow (of course, it does change at any fixed position in phase space).  As the number of systems in the ensemble is fixed, the Liouville Theorem implies that the volume of the phase space occupied by the ensemble is fixed.  

This means that the entropy, being the log of the volume, cannot change either.  So how can the entropy grow if the microscopic equations of motion show that it does not?  A resolution to this paradox is to notice that, for systems in which statistical mechanics holds, the Hamiltonian Flow takes nice looking, civilized ensembles and turns them into convoluted shapes:

%Insert convoluted flow diagram

The average of smooth observables (i.e. the value of $f(q,p)$) does not distinguish between the average performed with the true $\rho(t)$ or the "coarse grained" $\widetilde{\rho}(t)$, as shown below.

%Insert convoluted flow diagram 2

$$\overline{f}=\int_{}^{}dqdpf(q,p)\rho(q,p,t)\simeq\int_{}^{}dqdpf(q,p)\widetilde{\rho}(q,p,t)$$

For all practical purposes, the entropy grows so long as we only look at observables that are very smooth.  Macroscopic observables don't care about the precise poition of the particles and tend to be smooth in phase space.  Systems whose Hamiltonian Flow have this property are called "\textbf{Mixing}."  It is easy to show that ergodic systems are mixing, but keep in mind that mixing systems are not necessarily ergodic.    

\chapter{Thermodynamics}

From the general property of entropy we can derive a massive amount of information about a system.  In fact, if the entropy as a function of extensive variables is known, everything else about the thermodynamic behavior of the system can be found!  The definition of entropy is reiterated below:

$$S=\bltz\ln(\Gamma)$$

We include the Boltzmann Constant, $\bltz$, for historical reasons.  However, if we judiciously choose our units (by measuring temperature in units of energy, as we should) we can set $\bltz=1$.  A good thing to remember in this regard is that $10,000K\simeq1eV$.  

Since entropy is so important, let's review a few properties satisfied by the entropy:

\begin{itemize}
	\item S as a function of the \underline{extensive} variables contains all the information about a system that you need.
	\item S is itself extensive: $S(\lambda*E,\lambda*V,\lambda*N)=\lambda*S(E,V,N)$
	\item $\pardif{S}{E}\geq0$  We will eventually see that this implies $T\geq0$
	\item $S(E,V,N)$ is a concave function.  We will eventually see that this implies stability: \\
	$S(E+\Delta*E,V+\Delta*V,N+\Delta*N)+S(E-\Delta*E,V-\Delta*V,N-\Delta*N)\leq2S(E,V,N)$\\ 
		%insert plot of S showing concavity
	\item $S=0$ at $\frac{1}{\pardif{S}{E}}=0 (T=0)$  This property depends on how we normalize the number of microstates, $\Gamma$.  Quantum mechanics determines the right normalization that implies $S(T=0)-0$
	\item $S$ is maximized in equilibrium within the constraints set by the physical setup.  This is a direct consequence of the fact that all microstates are equally probable.
\end{itemize}

Inverting the entropy function $S(E,V,N)$ allows us to obtain a function for the energy of the system in terms of entropy: $E(S,V,N)$.  If $S,V,N$ are changed slowly (i.e. \textbf{"adiabatically"}) so that the system stays in equilibrium at all intermediate states we find:

$$dE=\thermdif{E}{S}{V}{N}dE+\thermdif{S}{V}{E}{N}dV+\thermdif{S}{N}{E}{V}dN$$

where

$$\thermdif{S}{E}{V}{N}=\frac{1}{T}, \thermdif{S}{V}{E}{N}=\frac{P}{T}, \thermdif{S}{N}{E}{V}=\frac{\mu}{T}$$

To derive the first relation, we used $\thermdif{E}{S}{V}{N}=\frac{1}{den}$

%Finish equation of state section

It's important to note that S is defined for equilibrium macroscopic states only.  So dS or dE refer to changes slow enough such that the system continuously remains in equilibrium during its transition from its initial state to its final state.  i.e., at any given time during the transition the system is in an equilibrium state.  For instance, if the volume is changed suddenly and then the system is left to relax to equilibrium the change in entropy is not given by $\frac{p}{T}dV$ (it will, in fact, be larger). 

From these relations we can derive the Equations of State:

\begin{tabu} to 0.8\textwidth { | X[l] | X[r] | }
	\hline
	$T=\thermdif{E}{S}{V}{N}=T(S,V,N)$ & $\frac{1}{T}=\thermdif{S}{E}{V}{N}=\frac{1}{T(E,V,N)}$ \\
	\hline
	$-P=\thermdif{E}{V}{S}{N}=P(S,V,N)$ & $-\frac{P}{T}=\thermdif{S}{V}{E}{N}=-\frac{P(E,V,N)}{T(E,V,N)}$ \\
	\hline
	$\mu=\thermdif{E}{N}{S}{V}=\mu(S,V,N)$ & $\frac{\mu}{T}=\thermdif{S}{N}{E}{V}=\frac{\mu(E,V,N)}{T(E,V,N)}$ \\
	\hline
\end{tabu}

These equations of state are particularly important because they are frequently accessible to experiments.  Notice that the same quantities as functions of other variables are \underline{not} equations of state and may not contain the same information. 

As mentioned earlier, all thermodynamical problems can be reduced to a transition between two states of a system:

%Insert diagram

Depending on the properties of the wall between the two subsystems, we find that:

\begin{itemize}
	\item Wall allows Energy exchange $\rightarrow T_{1}' = T_{2}'$
	\item Wall allows Volume exchange $\rightarrow V_{1}' = V_{2}'$
	\item Wall allows Particle exchange $\rightarrow \mu_{1}' = \mu_{2}'$
\end{itemize}

\section{Non-Relativistic, Non-Interacting, Monoatomic Gas}

Let us compute the entropy, S, for one example:

$$\mathcal{H}(\mathbb{r}_{i},\mathbb{p}_{i})=\sumser{i=1}{N}\frac{\mathbb{p}_{i}^{2}}{2m}, (0<\mathbb{r}_{i}<L)$$

\begin{equation}
\begin{split}
\Gamma(E,V,N)=\int\prod_{i=1}^{N}d\mathbb{r}_{i}\mathbb{p}_{i}\delta(E-sumser{i=1}{N}\frac{\mathbb{p}_{i}^{2}}{2m}) \\ 
	=[\int_{0}^{L}dx]^{3N}\int_{-\infty}^{\infty}dp_{ix}dp_{iy}dp_{iz}....dp_{nx}dp_{ny}dp_{nz}*2m\delta(2mE-\sumser{i=1}{3N}p_{i}^2) \\
	=V^{N}2mS^{3N}(R=\sqrt{2mE}) \\
	=2mV^{N}\frac{2\pi^{\frac{3N}{2}}}{\Gamma(\frac{3N}{2})}(2mE)^{\frac{3N-1}{2}} 
\end{split}
S=\bltz\ln{\Gamma}=\bltz[N\ln{V}+\frac{3N-1}{2}\ln{E}-\ln{\Gamma(\frac{3N}{2})}+\frac{3N}{2}\ln{\sum}+Constant]\overrightarrow{N\rightarrow\infty}\bltz[N\ln{V}+\frac{3N}{2}\ln{E}-\frac{3N}{2}\ln{N}+\frac{3}{2}N\ln{\prod}]=N\bltz\ln{\frac{VE^{\frac{3}{2}}}{N^{\frac{3}{2}}}}
\end{equation}
The above expression violates extensivity (2) and the Neust Postulate (5).  The violation of the Neust Postualte is not surprising: the behavior of a gas at T=0 is dictated by quantum mechanics, not classical mechanics (the atoms wavelength is much larger than the separation between them).  What is surprising is that quantum mechanics connects the calculation above even at large T.  What is missing from the classical calculation above is that the gas is composed of \underline{identical} particles (bosons or fermions, it doesn't matter at high T).  So the correct way of counting states is:

$$S=\bltz\ln{\Gamma}=\bltz\int\prod_{i=1}^{N}\frac{dq_{i}dp_{i}}{N!h^N}\delta(E-\sumser{i=1}{N}\frac{\mathbb{p}_{i}^2}{2m})$$

The denominator factor of $N!$ avoids double counting of microstates with the same particle position and the factor of $h^{N}$ becomes important when considering the classical limit of quantum gases.

As the number of particles in the system becomes very large we find that the expression becomes

$$S=\bltz*N\ln{\frac{VE^{\frac{3}{2}}}{N^{\frac{5}{2}}}}$$

From this entropy we can find the equations of state:

\begin{tabu} to 0.8\textwidth { | X[l] | X[r] | }
	\hline
	$\frac{1}{T}=\thermdif{S}{E}{V}{N}=\frac{3}{2}\frac{\bltz*N}{E}$ & $E=\frac{3}{2}\bltz*N*T$ \\
	\hline
	$-\frac{P}{T}=\thermdif{S}{V}{E}{N}=-\frac{\bltz*N}{V}$ & $PV=\bltz*NT$ \\
	\hline
	$\frac{\mu}{T}=\thermdif{S}{N}{E}{V}=\bltz\ln{\frac{VE^{\frac{3}{2}}}{N^{\frac{5}{2}}}}-\frac{5}{2}\bltz$ & $ $ \\
	\hline
\end{tabu}

Note that we found the Boyle-Mariott law without even discussing atoms colliding against walls or other kinetic theory considerations.  There's a lot behind the assumption of ergodicity.  

This leads us to....

\section{Euler Relation}

$$S(\lambda*E,\lambda*V,\lambda*N)=\lambda*S(E,V,N)$$

Taking the derivative of this expression in terms of $\lambda$ yields

$$E()\pardif{S}{E}=\frac{1}{T})+V(\pardif{S}{V}=-\frac{P}{T})+N(\pardif{S}{N}=\frac{\mu}{T})=S$$

Which can be re-arranged to yield the following equations:

$$E=TS-PV+\mu*N$$
$$S=\frac{E}{T}-\frac{PV}{T}+\frac{\mu*N}{T}$$

\subsection{Recovering the Entropy or Energy from an Equation of State}

If three equations of state are availible:

$$T(S,V,N)=\thermdif{E}{S}{V}{N}$$
$$-P(S,V,N)=\thermdif{E}{V}{S}{N}$$
$$\mu(S,V,N)=\thermdif{E}{N}{S}{V}$$

$$\rightarrow E(S,V,N)=T(S,V,N)=T(S,V,N)S-P(S,V,N)V+\mu(S,V,N)N$$

If only two equations of state are availible: $E(S,V,N)=NE(\frac{S}{N},\frac{V}{N},1)$

The equality above is due to extensivity.

$$T=\thermdif{E}{S}{V}{N}=T(S,V,N)=T(\frac{S}{N},\frac{V}{N},1)$$

$$-P=\thermdif{E}{V}{S}{N}=P(S,V,N)=P(\frac{S}{N},\frac{V}{N},1)$$

Note that there is no N!.  T and P are intensive.

$$\downarrow$$

$$E(\frac{S}{V},\frac{V}{N},1)=\int_{0}^{\frac{S}{N}}d(\frac{S}{N})T(\frac{S}{N},\frac{V}{N},1)+E(0,\frac{V}{N},1)=\int_{0}^{\frac{S}{N}}d(\frac{S}{N})T(\frac{S}{N},\frac{V}{N},1)-\int_{0}^{\frac{V}{N}}d(\frac{V}{N})P(0,\frac{V}{N},1)+E(0,0,1)$$

In the above expressions, $T(\frac{S}{N},\frac{V}{N},1)=\thermdif{E}{S}{V}{N}$ and $E(0,0,1)=E(\frac{S}{N},\frac{V}{N},1)$.  The latter equality can be found up to one constant independant of S,V, or N.

Now, if only one equation of state is availible, you cannot recover the entropy or energy.  There is simply not enough information in a single equation of state to extract these system properties.  

\subsection{EXAMPLE: Non-relativistic, Monoatomic, Non-interacting Gas}

$$S=\bltz*N\ln{\frac{VE^{\frac{3}{2}}}{N^{\frac{5}{2}}}}, E=(\frac{N^{\frac{5}{2}}}{V}e^{\frac{S}{\bltz*N}})^\frac{2}{3}=\frac{N^{\frac{5}{2}}}{V^{\frac{2}{3}}}*e^{\frac{2S}{3\bltz*N}}$$

$$T=\thermdif{E}{S}{V}{N}=\frac{2}{3\bltz*N}\frac{N^\frac{5}{3}}{V^\frac{2}{3}}e^\frac{2S}{3\bltz*N}$$

$$-P=\thermdif{E}{V}{S}{N}=-\frac{2}{3}\frac{N^\frac{5}{3}}{V^\frac{5}{3}}e^\frac{2S}{3\bltz*N}$$

$$E(\frac{S}{N},\frac{V}{N},1)=\int_{0}^{\frac{S}{N}}d(\frac{S}{N})\frac{2}{3\bltz}(\frac{N}{V})^{\frac{2}{3}}e^{\frac{2S}{3\bltz*N}}-\int_{0}^{\frac{V}{N}}d(\frac{V}{N})\frac{2}{3}(\frac{N}{V})^{\frac{5}{3}}+E(0,0,1)$$

$$E(\frac{S}{N},\frac{V}{N},1)=\frac{2}{3\bltz}(\frac{N}{V})^{\frac{2}{3}}\frac{3\bltz*N}{2}[e^{\frac{2S}{3\bltz*N}}-1]-\frac{2}{3}(\frac{-3}{2})(\frac{N}{V})^\frac{2}{3}$$

$$E(\frac{S}{N},\frac{V}{N},1)=\frac{N^{\frac{5}{3}}}{V^{\frac{1}{3}}}e^{\frac{2S}{3\bltz*N}}$$

\section{Minimum Energy Principle}

Before formally stating the minimum energy principle, let's begin with an analogy.  We know that a circle is the figure with the largest area for a given perimeter.  Of course, it is also the the figure with the smallest perimeter for a given area.  Even if a given circle was built by someone with a fixed length string trying to enclose the largest area it will still be true that the final circle minimizes the perimeter for the given area.  

The same thing is true for the maximum entropy principle.  After some constraints are changed the final state will be the one with the largest entropy for a given \textbf{total} energy (the constraint that the total energy is unchanged is always there).  But the final equilibrium state can also be characterized as the state with the smallest energy for a fixed entropy:

%Insert entropy surface diagrams

Convince yourself that S as a function of total E and subsystem energy $E_{1}^{1}$ has the general shape depicted above.  The minimum energy principle follows from the picture above.  

Now let's see how the minimum energy principle works in our archetypal example   :

%Insert thermo diagram

$$E=E_{1}(S_{1}^{1},V_{1},N_{1})+E_{2}(S_{2}^{'},V_{2},N_{2})$$

Where $S_{2}^{'}=S-S_{1}^{'}$.

Therefore, we find that the minimum energy, E, for fixed entropy, S, is:

$$\pardif{E}{S_{1}^{'}}=0 \rightarrow \pardif{E_{1}}{S_{1}^{'}}-\pardif{E_{2}}{S_{2}^{'}}=0 \rightarrow T_{1}=T_{2}$$

Which confirms what we expected: In its final state our example subsystem pair reaches an equilibrium temperature.

\section{Efficiency of Engines}

In our thermodynamic terminology, an 'engine' is a machine that extracts work from the the energy difference between two systems in contact.  To clarify, the engine takes energy from the 'hot' source, uses some of that energy to perform work, and ejects the remaining energy into the 'cold' sink.  In many cases, the 'cold' sink is the atmosphere: one example where this is true is a car engine, which throws off heat as it performs the work of moving the car.  

In designing our thermodynamic engine, we would like it to perform this energy transfer over and over again in a closed cycle.  I.e. we want the system to end up in the same macroscopic state as it started.  For our purposes right now we will assume that all changes in the system caused by the energy transfers occur slowly (adiabatically) so, that at every point in the cycle the machine is in equilibrium and its macroscopic state is specified by E,V,N or S,V,N.  It turns out, that for thermodynamic calculations it is better to use the variables T,S,N instead.  In our engine we want to keep N fixed by having impermeable walls separating the machine from the rest of the Universe.  This machine cycle can be represented by a closed loop in the TxS space:

%Insert engine cycle diagram

Let us start by considering one specific cycle, the \textbf{Carnot Cycle}:

%Insert Carnot Cycle/Piston diagrams

%Insert Carnot Cycle explaination diagram

For a fixed N: $dE=TdS-PdV$ so that 

$$\Delta Q_{1}=\int_{S_{1}}^{S_{2}}TdS=T_{h}(S_{2}-S_{1})$$
$$\Delta Q_{2}=\int_{S_{2}}^{S_{1}}TdS=-T_{c}(S_{2}-S_{1})$$

%Insert Carnot explaination diagram

Since after a full cycle the energy of the system goes back to what it was in the beginning we find:

$$\Delta Q_{1}+\Delta Q_{2}=\Delta W=$$ the area inside the cycles in the TxS plane.

In the expression above, $\Delta Q_{1}+\Delta Q_{2}$ is the net heat absorbed by the system and $\Delta W$ is the work done \textbf{by} the system.  

%Insert Carnot cycle deltaW diagram

For practical reasons, we would like to get as much work as possible from a given amount of heat absorbed.  The efficiency of the engine is defined as"

$$\epsilon=\frac{\Delta W}{\Delta Q_{1}}=\frac{(T_{h}-T_{c})(S_{2}-S_{1})}{T_{h}(S_{2}-S_{1})}=1-\frac{T_{c}}{T_{h}}<1$$

Now, one very important thing to note about this expression is that the total efficiency is \textit{less than} not \textit{less than or equal to} one because some of the energy is wasted and dispersed into the cold sink.  This begs the question: is there a way that we could change the cycle to obtain a larger efficiency? 

%Insert diagrams 

Going back to our TxS plane, we would want to maximize the area inside of the cycle ($\Delta W$) and minimize the area below the top curve ($\Delta Q_{1}$).  It's clear, for instance, that the cycle on the left can have its efficiency improved by changing it as shown below.  We end up then with the Carnot cycle, which is the best that can be done.  Thus, we have the universal result:

$$\epsilon\leq1-\frac{T_{c}}{T_{h}}$$  

Notice that we only used the fact that an ideal gas was used inside the engine to describe the different phases of the cycle in words.  The main result, $E\leq1-\frac{T_{c}}{T_{h}}$ does not depend on that and is valid universally.  

Finally, let us revisit the assumption of adiabaticity, namely, that all phases of the cycle were infinitely slow.  In each phase of the cycle the machine foes from the one initial state A to a final state B exchanging a certain amount of heat $\Delta Q$ (with a source at temperature T) and performing some work $\Delta W$ (positive or negative).  I'll assume the work is done on a reversible work source, namely, a system which can exchange work but not heat (a paper weight on top of a piston is an example).  Similarly, heat is exchanged with a reversible heat source.

%Insert heat exchange diagram

For given initial and final states A and B (and consequently, given changes in energy $E_{B}-E_{A}$ and entropy) the maximum amount of work will be delivered if $\Delta Q$ is a minimum as $E_{B}-E_{A}=-(\Delta W+\Delta Q)$.  The process with the smallest $\Delta Q$ will correspond to the smallest possible increase of the entropy of the universe (system+heat source, the work source has no entropy).  How much is the minimum amount of heat to be transferred to the heat source?  As the total entropy of the universe must increase (otherwise the system would stay at A) we have 

$$S_{B}-S_{A}+T_{r}\Delta Q\geq0$$

Where $T_{r}$ is the temperature of the heat source.  That means that the minimum amount of heat "wasted" on the heat source happens when the total entropy of the universe is unchanged as is given by 

$$\Delta Q=\frac{S_{A}-S_{B}}{T_{r}}$$

If more heat than $\frac{S_{A}-S_{B}}{T_{r}}$ is delivered to the heat source the universe's entropy will increase and less work will be delivered to the work source.  A similar analysis applies when the system is absorbing heat or work.  The main point is that \textbf{reversible processes}, those where the entropy of the universe does not change, lead to the maximum efficiency.  Thus, our bound on the efficiency of an engine obtained assuming adiabaticity, is not violated by non-adiabatic processes.  Notice that the adiabaticity condition means that the engine works really slowly.  Practical Machines, which need to do their job fast, are not adiabatic and much less efficient than the theoretical limit would expect (roughly a factor of 3 less efficient).

\section{Thermodynamic Stability}

Previously we determined the equilibrium condition between two subsystems: 

$$T_{1}=T_{2}$$ for a diathermal wall

$$P_{1}=P_{2}$$ for a moveable wall

$$\mu_{1}=\mu_{2}$$ for a porous wall

using the maximum entropy principle.  But we only imposed the $S=S_{1}+S_{2}$ was a critical point: it could have been a maximum, a minimum, or a saddle point.  We'll now see what the consequences are of demanding that the final equilibrium state is indeed a maximum.  

Let's consider first the case of two subsystems separated by a diathermal wall.  The condition that $S=S_{1}+S+{2}$ is a maximum implies:

%insert final state of equilibrium diagram

$$\dif{S}{E_{1}^{'}}=0\rightarrow\pardif{S_{1}}{E_{1}}=\pardif{S_{2}}{E_{2}}$$
$$\frac{d^{2}S}{dE_{1}^{2}}<0\rightarrow\frac{\delta^{2}S_{1}}{\delta E_{1}^{2}}+\frac{\delta^{2}S_{2}}{\delta E_{2}^{2}}<0$$

The quantity $\frac{\delta^{2}S}{\delta E^{2}}$ has a physical interpretation:

$$\secthermdif{S}{E}{V}{N}=\thermdif{\frac{1}{T}}{E}{V}{N}=-\frac{1}{T^{2}}\thermdif{T}{E}{V}{N}$$

$$=-\frac{1}{T^{2}}\frac{1}{\thermdif{E}{T}{V}{N}}=-\frac{1}{T^{2}N}\frac{1}{c_{v}}$$

Where $c_{v}$ is the system's specific heat at a constant volume.  From the above we see that the stability condition is then:

$$-\frac{1}{N_{1}T^{2}}\frac{1}{c_{v}^{1}}-\frac{1}{N_{2}T^{2}}\frac{1}{c_{v}^{2}}<0$$
or
$$\frac{1}{N_{1}c_{v}^{1}}+\frac{1}{N_{2}c_{v}^{2}}>0$$

This is true because $T_{1}=T_{2}$ in equilibrium.  By taking one subsystem to be much larger we conclude that each one of $c_{v}^{1}$ and $c_{v}^{2}$ should be positive.  This result is physically obvious.  Suppose $c_{v}^{1}<0$.  In this case, the system would get \textit{colder} by absorbing heat.  This would, of course, cause the system to absorb even more heat in a runaway cycle.  Clearly, this is an unstable situation.  

Next, let's consider the case where both E and V are exchanged between the subsystems.  We have:

$$0=\pardif{S}{E_{1}^{1}}=\pardif{S_{1}}{E_{1}}-\pardif{S_{2}}{E_{2}}$$
$$0=\pardif{S}{V_{1}^{1}}=\pardif{S_{1}}{V_{1}}-\pardif{S_{2}}{V_{2}}$$

$$0>d^{2}S=(dE_{1}^{1}dV_{1}^{1}) \left[ \begin{smallmatrix} \secpardif{S_{1}}{(E_{1}^{1})}&\frac{\delta^{2}S_{1}}{\delta E_{1}^{1}\delta V_{1}^{1}}\\ \frac{\delta^{2}S_{1}}{\delta E_{1}^{1}\delta V_{1}^{1}}&\secpardif{S_{1}}{(V_{1}^{1})} \end{smallmatrix} \right] 
\begin{bmatrix}
	dE_{1}^{1} \\
	dV_{1}^{1} 
\end{bmatrix}
+1\leftrightarrow2$$

We need that last term, 2, to be negative for any $dE_{1}^{1},dV_{1}^{1}$.  For the second variation $d^{2}S$ to be negative for any $E_{1}^{1},V_{1}^{1}$ it's necessary that the eigenvalues of 

$$\left[ \begin{smallmatrix} \secpardif{S_{1}}{(E_{1}^{2})}&\frac{\delta^{2}S_{1}}{\delta E_{1}^{1}\delta V_{1}^{1}}\\ \frac{\delta^{2}S_{1}}{\delta E_{1}^{1}\delta V_{1}^{1}}&\secpardif{S_{1}}{(V_{1}^{1})} \end{smallmatrix} \right]$$

to be negative.  So let's work out a general condition for this to be true.

$$\left| \begin{smallmatrix} a-\lambda&b\\ b&c-\lambda \end{smallmatrix} \right|=0$$

where

$$\lambda^{2}-(a+c)\lambda+ac-b^{2}=0$$
$$\lambda=\frac{a+c}{2}\pm\sqrt{\frac{a+c}{2}^{2}-(ac-b^{2})}<0$$
$$a+c<0 \& ac-b^{2}>0$$
$$a<0 \& c-\frac{b^{2}}{a}<0$$

In this case, $a=\secthermdif{S}{E}{V}{N}=\thermdif{\frac{1}{T}}{E}{V}{N}=-\frac{1}{T^{2}}\thermdif{T}{E}{V}{N}=-\frac{1}{T^{2}N}\frac{1}{c_{v}}<0$.  Note that the last part of the expression is the same condition as before, namely that $c_{v}>0$.

$$0>c-\frac{b^{2}}{a}=\secthermdif{S}{V}{E}{N}-(\frac{\delta^{2}S}{\delta E\delta V})^{2}\frac{1}{\secthermdif{S}{E}{V}{N}}$$

%Insert Diagram

Although this last expression is rather messy, we can rewrite in a more clear way with a transparent physical meaning.

$$\thermdif{\frac{P}{T}}{V}{T}{ }=\nabla(\frac{P}{T})*\mathbb{n}$$
$$\thermdif{\frac{P}{T}}{V}{T}{ }=\thermdif{\frac{P}{T}}{V}{E}{ }-\thermdif{T}{V}{E}{ }\frac{1}{\thermdif{T}{E}{V}{ }}\thermdif{\frac{P}{T}}{E}{V}{ }$$
$$\thermdif{\frac{P}{T}}{V}{T}{ }=\secpardif{S}{V}+T^{2}\frac{\delta^{2}S}{\delta E\delta V}\vert_{E}\frac{1}{-T^{2}\secthermdif{S}{E}{V}{ }}\frac{\delta^{2}S}{\delta V\delta E}$$  

So we find $\thermdif{\frac{P}{T}}{V}{T}{ }=\frac{1}{T}\thermdif{P}{V}{T}{ }<0$.  The $\frac{1}{T}\thermdif{P}{V}{T}{ }<0$ term can be rewritten to yield:

$$-\frac{1}{VT}[-\frac{1}{V}\thermdif{V}{P}{T}{ }]^{-1}=-\frac{1}{VT}\frac{1}{R_{T}}$$
	
We have defined a new quantity above, $R_{T}$.  The quantity is the \textbf{Isothermal Compressibility} of the gas.  The stability condition we defined earlier implies that the isothermal compressiblity is positive, however, this result is not immediately obvious.  A negative compressibility would mean that the system expands as the pressure increases.  This behaviour would increase the pressure of the other system leading to a runaway expansion of our observed system.

\chapter{Canonical Ensemble}

%Insert Canonical ensemble resevoir diagram

For many situations, it is much easier to treat a system as if it is in contact with an infinite heat reservoir.  This important property of this reservoir is that it is unaffected by any energy it absorbs due to its size.  In order to derive an ensemble to describe this system, let's start with the microcanonical ensemble:

$$\rho(q,p)~\Gamma(E-\mathcal{H}(q,p))$$

Where $q,p$ are the coordinates of the subsystem only, $\Gamma$ is the function which represents the number of microstates of the reservoir with energy $E-\mathcal{H}(q,p)$.  The terms within the gamma function represent the total energy of the reservoir (E) and the subsystem energy ($\mathcal{H}$).  Continuing our derivation:

$$\rho(q,p)~e^{\frac{1}{\bltz}*S_{R}(E-\mathcal{H}(q,p))}$$
Where $S_{R}$ is the reservoir's entropy.
$$\rho(q,p)\cong e^{\frac{1}{\bltz}S_{R}(E)-\frac{1}{\bltz}\mathcal{H}(q,p)\pardif{S_{R}}{E}}$$
In the expression above, we see that $\frac{1}{\bltz}S_{R}(E)$ is a constant independent of the coordinants $(q,p)$, and the $\pardif{S_{R}}{E}\rightarrow\frac{1}{T_{R}}=\frac{1}{T}$ term shows the equilibrium equivalence of the reservoir temperature ($T_{R}$) and the subsystem temperature ($T$).  Thus:

$$\rho(q,p)~e^{-\frac{\mathcal{H}(q,p)}{\bltz T}}$$

Therefore, we see that $\rho(q,p)$ is independent of the reservoir's properties.  This expression illustrates what is known as the \textbf{Canonical Ensemble}.

$$\rho(q,p)=\frac{e^{-\beta\mathcal{H}(q,p)}}{Z}$$
$$Z=\int dqdpe^{-\beta\mathcal{H}(q,p)}$$
$$\beta=\frac{1}{\bltz T}$$

\section{Equivalence with the Microcanonical Ensemble}

To see the equivalence of these two ensembles, let's start with the partition function:

$$Z=\int dqdpe^{-\beta\mathcal{H}(q,p)}=\int dE\Gamma(E)e^{-\beta E}$$

Where $dE\Gamma(E)$ is the number of microstates with energy between $E$ and $E+dE$.  Therefore

$$Z=\int dE e^{\frac{S(E)}{\bltz}-\beta E}$$

As we see in the above expression, $S(E)$ is the microcanonical ensemble and the entire exponent of $e$ is an extensive quantity which is $~N$ \textit{very} concentrated around the maximum of the integrand.

%Insert integral diagram

$$Z\cong\Delta e^{-\beta(E-TS(E))}\vert_{E=E*}$$

The energy constraint insists that the energy minimizes the value $E-TS(E)$.  This value has a particular significance and therefore has a specific name...

\section{Helmholtz Free Energy and the Partition Function}

As stated before, the quantity $E-TS(E,V,N)\vert_{E=E*}$ has a specific name: \textbf{Helmholtz Free Energy}, and is denoted by the function $F(T,V,N)$.  Before we further discuss the importance of this quantity, let's first explore the significance of the normalization factor Z in the Canonical Ensemble.  Z also has a specific name, the \textbf{Partition Function}.  It is extremely useful in its own right:

$$\pardif{\ln{Z}}{\beta}=\frac{1}{T}\pardif{Z}{\beta}=\frac{1}{Z}\int dqdp(-\mathcal{H})e^{-\beta\mathcal{H}(q,p)}=<-E>$$

Where $<-E>$ is the average of the energy ion the canonical ensemble.

$$\secpardif{\ln{Z}}{\beta}=\frac{\delta}{\delta\beta}\frac{1}{Z}\pardif{Z}{\beta}=-\frac{1}{Z^{2}}(\pardif{Z}{\beta})^{2}+\frac{1}{Z}\secpardif{Z}{\beta}$$

$$\secpardif{\ln{Z}}{\beta}=-<E>^{2}+\frac{1}{Z}\int dqdp\mathcal{H}^{2}(q,p)e^{-\beta\mathcal{H}(q,p)}$$

$$\secpardif{\ln{Z}}{\beta}=-<E>^{2}+<E^{2}>$$

$$\secpardif{\ln{Z}}{\beta}=<(E-<E>)^{2}>$$

However, keep in mind that 

$$\frac{\sqrt{<(E-<E>)^{2}}}{<E>}~\frac{\sqrt{N}}{N}~\frac{1}{\sqrt{N}}\rightarrow_{N>>1}0$$

Where $\sqrt{<(E-<E>)^{2}}$ is the fluctuation in energy and the $<E>$ is the average energy.

This shows that in the thermodynamic limit ($N\rightarrow\infty$) the energy does not fluctuate and only the microstates with an energy equal to the average value contributes significantly when computing averages using the canonical ensemble.  In other words, the canonical ensemble is equivalent to the microcanonical ensemble.

%Insert E/\rho diagram 

There are two specific situations where the canonical ensemble is used:

\begin{itemize}
	\item The system is coupled to a heat reservoir
	\item The system is isolated but it is easier to compute averages in the canonical ensemble.  This ends up being a very common situation
\end{itemize}

Previously we have argued that the averages of observables computed in the canonical ensemble are the same (in the Thermodynamic Limit) as the microcanonical ensemble.  Now, we would like to demonstrate that the information contained in $S(E,V,N)$ (or, equivalently, E(S,V,N)) is also contained in the Helmholtz Free Energy, $F(T,V,N)$(notice the arguments T,V,N; \textbf{NOT} S,V,N!).  To do this, we'll have to utilize a mathematical algorithm known as the Legendre Transform.

\subsection{Legendre Transforms}

Suppose $f(x)$ is not known but $f(p)$, where $p=\dif{f}{x}$, \textit{is} known.  Can we recover the function $f(x)$ from $f(p)$?  \textbf{NO!}  This is clearly illustrated by the figure below:


     

\end{document}